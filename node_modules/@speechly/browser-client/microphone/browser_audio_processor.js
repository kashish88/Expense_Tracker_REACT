"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
const types_1 = require("./types");
const sampler_1 = require("./sampler");
const audioProcessEvent = 'audioprocess';
const baseBufferSize = 4096;
class BrowserAudioProcessor {
    constructor(sampleRate, onAudio, sampler) {
        this.initialized = false;
        this.muted = false;
        this.handleAudio = (event) => {
            if (this.muted) {
                return;
            }
            this.onAudio(this.sampler.call(event.inputBuffer.getChannelData(0)));
        };
        try {
            const constraints = window.navigator.mediaDevices.getSupportedConstraints();
            this.nativeResamplingSupported = constraints.sampleRate === true;
        }
        catch (_a) {
            this.nativeResamplingSupported = false;
        }
        if (window.AudioContext !== undefined) {
            const opts = {};
            if (this.nativeResamplingSupported) {
                opts.sampleRate = sampleRate;
            }
            this.audioContext = new window.AudioContext(opts);
            this.isWebkit = false;
        }
        else if (window.webkitAudioContext !== undefined) {
            // eslint-disable-next-line new-cap
            this.audioContext = new window.webkitAudioContext();
            this.isWebkit = true;
        }
        else {
            throw types_1.ErrDeviceNotSupported;
        }
        this.sampler = sampler !== null && sampler !== void 0 ? sampler : sampler_1.newSampler(this.audioContext.sampleRate, sampleRate);
        this.sampleRate = sampleRate;
        this.onAudio = onAudio;
    }
    initialize() {
        var _a;
        return __awaiter(this, void 0, void 0, function* () {
            if (((_a = window.navigator) === null || _a === void 0 ? void 0 : _a.mediaDevices) === undefined) {
                throw types_1.ErrDeviceNotSupported;
            }
            // Start audio context if we are dealing with a WebKit browser.
            //
            // WebKit browsers (e.g. Safari) require to resume the context first,
            // before obtaining user media by calling `mediaDevices.getUserMedia`.
            //
            // If done in a different order, the audio context will resume successfully,
            // but will emit empty audio buffers.
            if (this.isWebkit) {
                yield this.audioContext.resume();
            }
            try {
                const opts = {
                    video: false,
                };
                if (this.nativeResamplingSupported) {
                    opts.audio = {
                        sampleRate: this.sampleRate,
                    };
                }
                else {
                    opts.audio = true;
                }
                this.mediaStream = yield window.navigator.mediaDevices.getUserMedia(opts);
            }
            catch (_b) {
                throw types_1.ErrNoAudioConsent;
            }
            this.audioTrack = this.mediaStream.getAudioTracks()[0];
            this.audioTrack.enabled = false;
            // Start audio context if we are dealing with a non-WebKit browser.
            //
            // Non-webkit browsers (currently only Chrome on Android)
            // require that user media is obtained before resuming the audio context.
            //
            // If audio context is attempted to be resumed before `mediaDevices.getUserMedia`,
            // `audioContext.resume()` will hang indefinitely, without being resolved or rejected.
            if (!this.isWebkit) {
                yield this.audioContext.resume();
            }
            if (this.isWebkit) {
                // Multiply base buffer size of 4 kB by the resample ratio rounded up to the next power of 2.
                // i.e. for 48 kHz to 16 kHz downsampling, this will be 4096 (base) * 4 = 16384.
                const bufSize = baseBufferSize * Math.pow(2, Math.ceil(Math.log(this.sampler.resampleRatio) / Math.log(2)));
                this.audioProcessor = this.audioContext.createScriptProcessor(bufSize, 1, 1);
            }
            else {
                this.audioProcessor = this.audioContext.createScriptProcessor(undefined, 1, 1);
            }
            this.audioContext.createMediaStreamSource(this.mediaStream).connect(this.audioProcessor);
            this.audioProcessor.connect(this.audioContext.destination);
            this.audioProcessor.addEventListener(audioProcessEvent, this.handleAudio);
            this.initialized = true;
        });
    }
    close() {
        return __awaiter(this, void 0, void 0, function* () {
            if (!this.initialized) {
                throw types_1.ErrNotInitialized;
            }
            // Stop all media tracks
            const stream = this.mediaStream;
            stream.getTracks().forEach(t => t.stop());
            // Disconnect and stop ScriptProcessorNode
            const proc = this.audioProcessor;
            proc.disconnect();
            proc.removeEventListener(audioProcessEvent, this.handleAudio);
            // Unset all audio infrastructure
            this.mediaStream = undefined;
            this.audioTrack = undefined;
            this.audioProcessor = undefined;
            this.initialized = false;
        });
    }
    mute() {
        this.muted = true;
        if (this.initialized) {
            const t = this.audioTrack;
            t.enabled = false;
        }
    }
    unmute() {
        this.muted = false;
        if (this.initialized) {
            const t = this.audioTrack;
            t.enabled = true;
        }
    }
}
exports.BrowserAudioProcessor = BrowserAudioProcessor;
//# sourceMappingURL=browser_audio_processor.js.map